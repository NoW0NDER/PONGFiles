{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "resume = False\n",
    "render = True\n",
    "save_freq = 500\n",
    "resume_pkl = \"Q.pkl\"\n",
    "max_episodes = 100\n",
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# env = gym.make('ALE/Pong-v5', render_mode='rgb_array')\n",
    "env = gym.make('PongDeterministic-v4', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 0, 'episode_frame_number': 0, 'frame_number': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obv = env.reset()\n",
    "obv[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "cumulated_reward = 0\n",
    "\n",
    "# frames = []\n",
    "# for t in range(1000):\n",
    "# #     print(observation)\n",
    "#     frames.append(env.render())\n",
    "#     # very stupid agent, just makes a random action within the allowd action space\n",
    "#     action = env.action_space.sample()\n",
    "# #     print(\"Action: {}\".format(t+1))\n",
    "#     observation, reward, done, truncated, info = env.step(action)\n",
    "# #     print(reward)\n",
    "#     cumulated_reward += reward\n",
    "#     if done:\n",
    "#         print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
    "#         break\n",
    "# print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frame in frames:\n",
    "#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     cv2.imshow('frame', gray_frame)\n",
    "#     cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames[200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greyframe = cv2.cvtColor(frames[300], cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greyframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(cv2.cvtColor(frames[200][34:], cv2.COLOR_BGR2GRAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gray_frame = cv2.cvtColor(frames[-1][34:-16], cv2.COLOR_BGR2GRAY)\n",
    "# #shape: (210-25=185 , 160)\n",
    "\n",
    "# unique_values = [64, 123, 147, 236]\n",
    "# coordinates = {}\n",
    "\n",
    "# for value in unique_values:\n",
    "#     coordinates[value] = np.where(gray_frame == value)\n",
    "\n",
    "\n",
    "# # for value, (x, y) in coordinates.items():\n",
    "# #     print(f\"Value: {value}, Coordinates: {list(zip(x, y))}\")\n",
    "# #     print(f\"Value: {value}, Coordinates: {len(list(zip(x, y)))}\")\n",
    "# #     print()\n",
    "# # #bg\n",
    "# print(f\"Value: {64}, Coordinates: {(coordinates[64][0][0]+coordinates[64][0][-1])/2, (coordinates[64][1][0]+coordinates[64][1][-1])/2}\")\n",
    "# #left\n",
    "# print(f\"Value: {123}, Coordinates: {(coordinates[123][0][0]+coordinates[123][0][-1])/2, (coordinates[123][1][0]+coordinates[123][1][-1])/2}\")\n",
    "# #right\n",
    "# print(f\"Value: {147}, Coordinates: {(coordinates[147][0][0]+coordinates[147][0][-1])/2, (coordinates[147][1][0]+coordinates[147][1][-1])/2}\")\n",
    "# #ball\n",
    "# print(f\"Value: {236}, Coordinates: {(coordinates[236][0][0]+coordinates[236][0][-1])/2, (coordinates[236][1][0]+coordinates[236][1][-1])/2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# done = False\n",
    "# no_steps = 0\n",
    "# while not done:\n",
    "#     a = random.randint(0, 5)\n",
    "#     state, reward, done,tun, info = env.step(a)\n",
    "#     no_steps += 1\n",
    "\n",
    "# print(no_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretizer(state):\n",
    "    greyimg = cv2.cvtColor(state[34:-16], cv2.COLOR_BGR2GRAY)\n",
    "    unique_values = [123, 147, 236]\n",
    "    coordinates = {}\n",
    "    for value in unique_values:\n",
    "        coordinates[value] = np.where(greyimg == value)\n",
    "    #left center\n",
    "    #print(coordinates[123], coordinates[147], coordinates[236])\n",
    "    if coordinates[123][0].any():\n",
    "        lc = (coordinates[123][0][0]+coordinates[123][0][-1])/2\n",
    "    else:\n",
    "        lc = -1\n",
    "    #right center\n",
    "    if coordinates[147][0].any():\n",
    "        rc = (coordinates[147][0][0]+coordinates[147][0][-1])/2\n",
    "    else:\n",
    "        rc = -1\n",
    "    #ball center\n",
    "    if coordinates[236][0].any():\n",
    "        bc = ((coordinates[236][0][0]+coordinates[236][0][-1])/2, (coordinates[236][1][0]+coordinates[236][1][-1])/2)\n",
    "    else:\n",
    "        bc = (-1,-1)\n",
    "    \n",
    "    return int(lc), int(rc), int(bc[0]),int(bc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretizer(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames[200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretizer(frames[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([160//2, 160//2, 160//2, 160//2, 6],dtype=np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245760000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.nbytes\n",
    "#31457280000 default dtype \n",
    "#3932160000 unit8 \n",
    "#245760000 with//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state:tuple):\n",
    "    return np.argmax(Q[state])\n",
    "def new_Q_value(reward: float, state_new: tuple, discount_factor=1):\n",
    "    future_optimal_value = np.max(Q[state_new])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value\n",
    "\n",
    "def learning_rate(n:int, min_rate=0.01):\n",
    "    return max(min_rate, min(1.0, 1.0-math.log10((n+1)/25)))\n",
    "\n",
    "def exploration_rate(n:int, min_rate=0.1):\n",
    "    return max(min_rate, min(1, 1.0-math.log10((n+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(env.reset()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\10536\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "n_episodes = max_episodes\n",
    "\n",
    "if not resume:\n",
    "    for e in tqdm(range(n_episodes)):\n",
    "        # print(*env.reset())\n",
    "        # print(*env.reset[-1])\n",
    "        current_state, done = discretizer(env.reset()[0]), False\n",
    "        while done == False:\n",
    "            action = policy(current_state)\n",
    "            if np.random.random() < exploration_rate(e):\n",
    "                action = env.action_space.sample()\n",
    "            #print(env.step(action)[0].shape)\n",
    "            obs, reward, done,tun, _ = env.step(action)\n",
    "            # print(\"obs:\",obs[0])\n",
    "            new_state = discretizer(obs)\n",
    "            \n",
    "            lr = learning_rate(e)\n",
    "            learnt_value = new_Q_value(reward, new_state)\n",
    "            old_value = Q[current_state][action]\n",
    "            Q[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "            \n",
    "            current_state = new_state\n",
    "\n",
    "        # if e%save_freq == 0:\n",
    "        #     with open(\"Q%d.pkl\"%e,\"wb\") as f:\n",
    "        #         pickle.dump(Q,f)\n",
    "\n",
    "else:\n",
    "    frames = []\n",
    "    with open(resume_pkl,\"rb\") as f:\n",
    "        Q = pickle.load(f)\n",
    "    for e in tqdm(range(n_episodes)):\n",
    "        # print(*env.reset())\n",
    "        # print(*env.reset[-1])\n",
    "        current_state, done = discretizer(env.reset()[0]), False\n",
    "        while done == False:\n",
    "            action = policy(current_state)\n",
    "            if np.random.random() < exploration_rate(e):\n",
    "                action = env.action_space.sample()\n",
    "            #print(env.step(action)[0].shape)\n",
    "            obs, reward, done,tun, _ = env.step(action)\n",
    "            # print(\"obs:\",obs[0])\n",
    "            new_state = discretizer(obs)\n",
    "            \n",
    "            lr = learning_rate(e)\n",
    "            learnt_value = new_Q_value(reward, new_state)\n",
    "            old_value = Q[current_state][action]\n",
    "            Q[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "            \n",
    "            current_state = new_state\n",
    "            frames.append(obs)\n",
    "        # if e%save_freq == 0:\n",
    "        #     with open(\"Q%d.pkl\"%e,\"wb\") as f:\n",
    "        #         pickle.dump(Q,f)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3932160000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"Q.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Q, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    frames = []\n",
    "    env.reset()\n",
    "    Q = pickle.load(open(resume_pkl, \"rb\"))\n",
    "    # print(type(Q))\n",
    "    current_state, done = discretizer(env.reset()[0]), False\n",
    "    while done == False:\n",
    "        action = policy(current_state)\n",
    "        # print(action)\n",
    "        #print(env.step(action)[0].shape)\n",
    "        if np.random.random() < exploration_rate(100):\n",
    "            action = env.action_space.sample()\n",
    "        obs, reward, done,tun, _ = env.step(action)\n",
    "        # print(\"obs:\",obs[0])\n",
    "        new_state = discretizer(obs)\n",
    "\n",
    "\n",
    "        current_state = new_state\n",
    "        frames.append(obs)\n",
    "    for frame in frames:\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\WorkSpace\\HKU\\machine learning\\PONG\\qlearning.ipynb 单元格 27\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m frame \u001b[39min\u001b[39;00m frames:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m, frame)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if render:\n",
    "    for frame in frames:\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(17, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()  # 将输入数据转换为float类型\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 创建一个网络实例\n",
    "net = Net()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\WorkSpace\\HKU\\machine learning\\PONG\\qlearning.ipynb 单元格 32\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m             frame_no \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m             cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39masarray(obs))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m train(\u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32mc:\\WorkSpace\\HKU\\machine learning\\PONG\\qlearning.ipynb 单元格 32\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     actions \u001b[39m=\u001b[39m [net(torch\u001b[39m.\u001b[39mcat([frames[\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m], frames[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m], frames[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], frames[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], torch\u001b[39m.\u001b[39mtensor([i])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)])) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m6\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     action \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(actions))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n",
      "\u001b[1;32mc:\\WorkSpace\\HKU\\machine learning\\PONG\\qlearning.ipynb 单元格 32\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     actions \u001b[39m=\u001b[39m [net(torch\u001b[39m.\u001b[39;49mcat([frames[\u001b[39m-\u001b[39;49m\u001b[39m4\u001b[39;49m], frames[\u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m], frames[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m], frames[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], torch\u001b[39m.\u001b[39;49mtensor([i])\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)])) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m6\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     action \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(actions))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/WorkSpace/HKU/machine%20learning/PONG/qlearning.ipynb#X44sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "# net = net.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "def train(n_eposides):\n",
    "    for e in tqdm(range(n_eposides)):\n",
    "        frame_no = 0\n",
    "        frames = deque(maxlen=16)\n",
    "        env.reset()\n",
    "        current_state, done = discretizer(env.reset()[0]), False\n",
    "        while not done:\n",
    "            if frame_no < 4:\n",
    "                action = env.action_space.sample()\n",
    "                obs, reward, done,_, __ = env.step(action)\n",
    "                for i in range(4):\n",
    "                    frames.append(current_state[i])\n",
    "                frame_no += 1\n",
    "                continue\n",
    "            else:\n",
    "                actions = [net(torch.tensor(list(frames+[i]))) for i in range(6)]\n",
    "                action = actions.index(max(actions))\n",
    "            for i in range(4):\n",
    "                frames.append(torch.tensor(current_state[i])) \n",
    "            obs, reward, done, _,__ = env.step(action)\n",
    "            current_state = discretizer(obs)\n",
    "            optimizer.zero_grad()\n",
    "            predicted_reward = net(torch.tensor(list(frames+[i])))\n",
    "            loss = criterion(predicted_reward, torch.tensor([reward]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            frame_no += 1\n",
    "            cv2.imshow('frame', np.asarray(obs))\n",
    "train(20)\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你已经有了状态信息和实际奖励\n",
    "states = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "actual_reward = torch.tensor([10], dtype=torch.float32)\n",
    "\n",
    "# 清除之前计算的梯度\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 步骤1：使用网络生成预测奖励\n",
    "predicted_reward = net(states)\n",
    "\n",
    "# 步骤2：计算预测奖励和实际奖励之间的损失\n",
    "loss = criterion(predicted_reward, actual_reward)\n",
    "\n",
    "# 步骤3：使用loss.backward()计算损失相对于模型参数的梯度\n",
    "loss.backward()\n",
    "\n",
    "# 步骤4：使用optimizer.step()更新模型参数\n",
    "optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
